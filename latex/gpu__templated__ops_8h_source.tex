\doxysection{gpu\+\_\+templated\+\_\+ops.\+h}
\hypertarget{gpu__templated__ops_8h_source}{}\label{gpu__templated__ops_8h_source}\index{gpu\_templated\_ops.h@{gpu\_templated\_ops.h}}

\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00001}00001\ \textcolor{preprocessor}{\#ifndef\ GPU\_TEMPLATED\_OPS\_H}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00002}00002\ \textcolor{preprocessor}{\#define\ GPU\_TEMPLATED\_OPS\_H}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00003}00003\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00004}00004\ \textcolor{preprocessor}{\#include\ "{}plumbing/defs.h"{}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00005}00005\ \textcolor{preprocessor}{\#include\ "{}plumbing/backend\_gpu/defs.h"{}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00006}00006\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00007}00007\ \textcolor{comment}{//\ this\ include\ has\ to\ be\ after\ the\ backend\ defs,\ because\ those\ define\ hila::random()}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00008}00008\ \textcolor{preprocessor}{\#include\ "{}plumbing/random.h"{}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00009}00009\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00010}00010\ \textcolor{preprocessor}{\#include\ "{}plumbing/type\_tools.h"{}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00011}00011\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00012}00012\ \textcolor{comment}{//\ \#if\ defined(\_\_CUDACC\_\_)\ ||\ defined(\_\_HIPCC\_\_)}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00013}00013\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00014}00014\ \textcolor{preprocessor}{\#if\ !defined(HILAPP)}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00015}00015\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00016}00016\ \textcolor{comment}{/*\ Reduction\ */}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00017}00017\ \textcolor{comment}{/*}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00018}00018\ \textcolor{comment}{template<typename\ T>}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00019}00019\ \textcolor{comment}{T\ cuda\_reduce\_sum(\ \ T\ *\ vector,\ int\ N\ )\{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00020}00020\ \textcolor{comment}{\ \ static\ bool\ initialized\ =\ false;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00021}00021\ \textcolor{comment}{\ \ static\ T\ *\ d\_sum;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00022}00022\ \textcolor{comment}{\ \ static\ void\ *d\_temp\_storage;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00023}00023\ \textcolor{comment}{\ \ static\ size\_t\ temp\_storage\_size;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00024}00024\ \textcolor{comment}{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00025}00025\ \textcolor{comment}{\ \ T\ sum;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00026}00026\ \textcolor{comment}{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00027}00027\ \textcolor{comment}{\ \ if(\ !initialized\ )\ \{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00028}00028\ \textcolor{comment}{\ \ \ \ cudaMalloc(\ \&d\_sum,\ sizeof(T)\ );}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00029}00029\ \textcolor{comment}{\ \ \ \ d\_temp\_storage\ =\ NULL;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00030}00030\ \textcolor{comment}{\ \ \ \ temp\_storage\_size\ =\ 0;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00031}00031\ \textcolor{comment}{\ \ \ \ cub::DeviceReduce::Sum(d\_temp\_storage,\ temp\_storage\_size,\ vector,\ d\_sum,\ N);}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00032}00032\ \textcolor{comment}{\ \ \ \ initialized\ =\ true;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00033}00033\ \textcolor{comment}{\ \ \}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00034}00034\ \textcolor{comment}{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00035}00035\ \textcolor{comment}{\ \ //\ Allocate\ temporary\ storage}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00036}00036\ \textcolor{comment}{\ \ cudaMalloc(\&d\_temp\_storage,\ temp\_storage\_size);}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00037}00037\ \textcolor{comment}{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00038}00038\ \textcolor{comment}{\ \ //\ Run\ sum-\/reduction}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00039}00039\ \textcolor{comment}{\ \ cub::DeviceReduce::Sum(d\_temp\_storage,\ temp\_storage\_size,\ vector,\ d\_sum,\ N);}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00040}00040\ \textcolor{comment}{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00041}00041\ \textcolor{comment}{\ \ cudaMemcpy(\ \&sum,\ d\_sum,\ sizeof(T),\ cudaMemcpyDeviceToHost\ );}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00042}00042\ \textcolor{comment}{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00043}00043\ \textcolor{comment}{\ \ return\ sum;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00044}00044\ \textcolor{comment}{\}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00045}00045\ \textcolor{comment}{*/}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00046}00046\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00047}00047\ \textcolor{comment}{//\ A\ simple\ hand-\/written\ reduction\ that\ does\ not\ require\ a\ library}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00048}00048\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00049}00049\ \_\_global\_\_\ \textcolor{keywordtype}{void}\ gpu\_reduce\_sum\_kernel(T\ *vector,\ \textcolor{keywordtype}{int}\ vector\_size,\ \textcolor{keywordtype}{int}\ new\_size,\ \textcolor{keywordtype}{int}\ elems)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00050}00050\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ Index\ =\ threadIdx.x\ +\ blockIdx.x\ *\ blockDim.x;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00051}00051\ \ \ \ \ \textcolor{keywordflow}{if}\ (Index\ <\ new\_size)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00052}00052\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 1;\ i\ <\ elems;\ i++)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00053}00053\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}\ ind\ =\ Index\ +\ i\ *\ new\_size;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00054}00054\ \ \ \ \ \ \ \ \ \ \ \ \ vector[Index]\ +=\ vector[ind];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00055}00055\ \ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00056}00056\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00057}00057\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00058}00058\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00059}00059\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00060}00060\ T\ gpu\_reduce\_sum(T\ *vector,\ \textcolor{keywordtype}{int}\ N)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00061}00061\ \ \ \ \ \textcolor{keyword}{const}\ \textcolor{keywordtype}{int}\ reduce\_step\ =\ 32;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00062}00062\ \ \ \ \ T\ sum\ =\ 0;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00063}00063\ \ \ \ \ T\ *host\_vector\ =\ (T\ *)memalloc(N\ *\ \textcolor{keyword}{sizeof}(T));}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00064}00064\ \ \ \ \ \textcolor{keywordtype}{int}\ vector\_size\ =\ N;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00065}00065\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00066}00066\ \ \ \ \ \textcolor{keywordflow}{while}\ (vector\_size\ >\ reduce\_step)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00067}00067\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Take\ the\ last\ n\ elements\ that\ are\ divisible\ by\ reduce\_step}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00068}00068\ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}\ first\ =\ vector\_size\ \%\ reduce\_step;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00069}00069\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Calculate\ the\ size\ of\ the\ reduced\ list}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00070}00070\ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}\ new\_size\ =\ (vector\_size\ -\/\ first)\ /\ reduce\_step;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00071}00071\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Find\ number\ of\ blocks\ and\ launch\ the\ kernel}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00072}00072\ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}\ blocks\ =\ (new\_size\ -\/\ 1)\ /\ N\_threads\ +\ 1;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00073}00073\ \ \ \ \ \ \ \ \ gpu\_reduce\_sum\_kernel<<<blocks,\ N\_threads>>>(vector\ +\ first,\ vector\_size,\ new\_size,}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00074}00074\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ reduce\_step);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00075}00075\ \ \ \ \ \ \ \ \ check\_device\_error(\textcolor{stringliteral}{"{}gpu\_reduce\_sum\ kernel"{}});}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00076}00076\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Find\ the\ full\ size\ of\ the\ resulting\ array}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00077}00077\ \ \ \ \ \ \ \ \ vector\_size\ =\ new\_size\ +\ first;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00078}00078\ \ \ \ \ \ \ \ \ gpuDeviceSynchronize();}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00079}00079\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00080}00080\ \ \ \ \ gpuMemcpy(host\_vector,\ vector,\ vector\_size\ *\ \textcolor{keyword}{sizeof}(T),\ gpuMemcpyDeviceToHost);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00081}00081\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00082}00082\ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ vector\_size;\ i++)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00083}00083\ \ \ \ \ \ \ \ \ sum\ +=\ host\_vector[i];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00084}00084\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00085}00085\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00086}00086\ \ \ \ \ free(host\_vector);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00087}00087\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00088}00088\ \ \ \ \ \textcolor{keywordflow}{return}\ sum;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00089}00089\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00090}00090\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00091}00091\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00092}00092\ \_\_global\_\_\ \textcolor{keywordtype}{void}\ gpu\_reduce\_product\_kernel(T\ *vector,\ \textcolor{keywordtype}{int}\ vector\_size,\ \textcolor{keywordtype}{int}\ new\_size,\ \textcolor{keywordtype}{int}\ elems)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00093}00093\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ Index\ =\ threadIdx.x\ +\ blockIdx.x\ *\ blockDim.x;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00094}00094\ \ \ \ \ \textcolor{keywordflow}{if}\ (Index\ <\ new\_size)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00095}00095\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 1;\ i\ <\ elems;\ i++)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00096}00096\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}\ ind\ =\ Index\ +\ i\ *\ new\_size;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00097}00097\ \ \ \ \ \ \ \ \ \ \ \ \ vector[Index]\ *=\ vector[ind];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00098}00098\ \ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00099}00099\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00100}00100\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00101}00101\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00102}00102\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00103}00103\ T\ gpu\_reduce\_product(T\ *vector,\ \textcolor{keywordtype}{int}\ N)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00104}00104\ \ \ \ \ \textcolor{keyword}{const}\ \textcolor{keywordtype}{int}\ reduce\_step\ =\ 32;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00105}00105\ \ \ \ \ T\ prod;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00106}00106\ \ \ \ \ prod\ =\ 1;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00107}00107\ \ \ \ \ T\ *host\_vector\ =\ (T\ *)malloc(N\ *\ \textcolor{keyword}{sizeof}(T));}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00108}00108\ \ \ \ \ \textcolor{keywordtype}{int}\ vector\_size\ =\ N;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00109}00109\ \ \ \ \ \textcolor{keywordflow}{while}\ (vector\_size\ >\ reduce\_step)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00110}00110\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Take\ the\ last\ n\ elements\ that\ are\ divisible\ by\ reduce\_step}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00111}00111\ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}\ first\ =\ vector\_size\ \%\ reduce\_step;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00112}00112\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Calculate\ the\ size\ of\ the\ reduced\ list}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00113}00113\ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}\ new\_size\ =\ (vector\_size\ -\/\ first)\ /\ reduce\_step;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00114}00114\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Find\ number\ of\ blocks\ and\ launch\ the\ kernel}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00115}00115\ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}\ blocks\ =\ new\_size\ /\ N\_threads\ +\ 1;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00116}00116\ \ \ \ \ \ \ \ \ gpu\_reduce\_product\_kernel<<<blocks,\ N\_threads>>>(vector\ +\ first,\ vector\_size,\ new\_size,}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00117}00117\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ reduce\_step);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00118}00118\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00119}00119\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Find\ the\ full\ size\ of\ the\ resulting\ array}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00120}00120\ \ \ \ \ \ \ \ \ vector\_size\ =\ new\_size\ +\ first;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00121}00121\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00122}00122\ \ \ \ \ \ \ \ \ gpuDeviceSynchronize();}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00123}00123\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00124}00124\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00125}00125\ \ \ \ \ check\_device\_error(\textcolor{stringliteral}{"{}gpu\_reduce\_product\ kernel"{}});}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00126}00126\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00127}00127\ \ \ \ \ gpuMemcpy(host\_vector,\ vector,\ vector\_size\ *\ \textcolor{keyword}{sizeof}(T),\ gpuMemcpyDeviceToHost);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00128}00128\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00129}00129\ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ vector\_size;\ i++)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00130}00130\ \ \ \ \ \ \ \ \ prod\ *=\ host\_vector[i];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00131}00131\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00132}00132\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00133}00133\ \ \ \ \ free(host\_vector);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00134}00134\ \ \ \ \ \textcolor{keywordflow}{return}\ prod;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00135}00135\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00136}00136\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00137}00137\ \textcolor{preprocessor}{\#if\ 0}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00138}00138\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00139}00139\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00140}00140\ \textcolor{keywordtype}{void}\ gpu\_multireduce\_sum(std::vector<T>\ \&vector,\ T\ *d\_array,\ \textcolor{keywordtype}{int}\ N)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00141}00141\ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ v\ =\ 0;\ v\ <\ vector.size();\ v++)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00142}00142\ \ \ \ \ \ \ \ \ vector[v]\ +=\ gpu\_reduce\_sum(d\_array\ +\ v\ *\ N,\ N);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00143}00143\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00144}00144\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00145}00145\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00146}00146\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00147}00147\ \textcolor{keywordtype}{void}\ gpu\_multireduce\_product(std::vector<T>\ vector,\ T\ *d\_array,\ \textcolor{keywordtype}{int}\ N)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00148}00148\ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ v\ =\ 0;\ v\ <\ vector.size();\ v++)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00149}00149\ \ \ \ \ \ \ \ \ vector[v]\ +=\ gpu\_reduce\_product(d\_array\ +\ v\ *\ N,\ N);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00150}00150\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00151}00151\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00152}00152\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00153}00153\ \textcolor{preprocessor}{\#endif}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00154}00154\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00160}00160\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00161}00161\ \textcolor{preprocessor}{\#if\ \_\_CUDA\_ARCH\_\_\ <\ 600}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00162}00162\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00163}00163\ \_\_device\_\_\ \textcolor{keyword}{inline}\ \textcolor{keywordtype}{double}\ atomic\_Add(\textcolor{keywordtype}{double}\ *dp,\ \textcolor{keywordtype}{double}\ v)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00164}00164\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00165}00165\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ *dp\_ull\ =\ (\textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ *)dp;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00166}00166\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ old\ =\ *dp\_ull;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00167}00167\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ av;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00168}00168\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00169}00169\ \ \ \ \ \textcolor{keywordflow}{do}\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00170}00170\ \ \ \ \ \ \ \ \ av\ =\ old;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00171}00171\ \ \ \ \ \ \ \ \ old\ =\ atomicCAS(dp\_ull,\ av,\ \_\_double\_as\_longlong(v\ +\ \_\_longlong\_as\_double(av)));}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00172}00172\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00173}00173\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Note:\ uses\ integer\ comparison\ to\ avoid\ hang\ in\ case\ of\ NaN\ (since\ NaN\ !=\ NaN)}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00174}00174\ \ \ \ \ \}\ \textcolor{keywordflow}{while}\ (av\ !=\ old);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00175}00175\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00176}00176\ \ \ \ \ \textcolor{keywordflow}{return}\ \_\_longlong\_as\_double(old);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00177}00177\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00178}00178\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00179}00179\ \textcolor{preprocessor}{\#else}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00180}00180\ \_\_device\_\_\ \textcolor{keyword}{inline}\ \textcolor{keywordtype}{double}\ atomic\_Add(\textcolor{keywordtype}{double}\ *dp,\ \textcolor{keywordtype}{double}\ v)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00181}00181\ \ \ \ \ \textcolor{keywordflow}{return}\ atomicAdd(dp,\ v);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00182}00182\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00183}00183\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00184}00184\ \textcolor{preprocessor}{\#endif}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00185}00185\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00187}00187\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00188}00188\ \_\_device\_\_\ \textcolor{keyword}{inline}\ \textcolor{keywordtype}{float}\ atomic\_Add(\textcolor{keywordtype}{float}\ *dp,\ \textcolor{keywordtype}{float}\ v)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00189}00189\ \ \ \ \ \textcolor{keywordflow}{return}\ atomicAdd(dp,\ v);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00190}00190\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00191}00191\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00192}00192\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00198}00198\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00199}00199\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T,\ \textcolor{keyword}{typename}\ B,}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00200}00200\ \ \ \ \ \ \ \ \ \ \ std::enable\_if\_t<std::is\_integral<T>::value\ \&\&\ \textcolor{keyword}{sizeof}(T)\ ==\ \textcolor{keyword}{sizeof}(\textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long})\ \&\&}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00201}00201\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ std::is\_convertible<B,\ T>::value,}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00202}00202\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{int}>\ =\ 0>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00203}00203\ \_\_device\_\_\ \textcolor{keyword}{inline}\ T\ atomicAdd(T\ *dp,\ B\ v)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00204}00204\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00205}00205\ \ \ \ \ T\ tv\ =\ v;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00206}00206\ \ \ \ \ \textcolor{keywordflow}{return}\ atomicAdd((\textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ *)dp,\ (\textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int})tv);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00207}00207\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00208}00208\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00211}00211\ \textcolor{keyword}{template}\ <}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00212}00212\ \ \ \ \ \textcolor{keyword}{typename}\ T,\ \textcolor{keyword}{typename}\ B,}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00213}00213\ \ \ \ \ std::enable\_if\_t<!std::is\_arithmetic<T>::value\ \&\&\ std::is\_convertible<B,\ T>::value,\ \textcolor{keywordtype}{int}>\ =\ 0>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00214}00214\ \_\_device\_\_\ \textcolor{keyword}{inline}\ \textcolor{keywordtype}{void}\ atomicAdd(T\ *d,\ \textcolor{keyword}{const}\ B\ \&bv)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00215}00215\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00216}00216\ \ \ \ \ T\ v\ =\ bv;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00217}00217\ \ \ \ \ hila::scalar\_type<T>\ *dp;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00218}00218\ \ \ \ \ \textcolor{keyword}{const}\ hila::scalar\_type<T>\ *dv;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00219}00219\ \ \ \ \ \textcolor{keyword}{constexpr}\ \textcolor{keywordtype}{int}\ N\ =\ \textcolor{keyword}{sizeof}(T)\ /\ \textcolor{keyword}{sizeof}(hila::scalar\_type<T>);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00220}00220\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00221}00221\ \ \ \ \ dp\ =\ (hila::scalar\_type<T>\ *)(\textcolor{keywordtype}{void}\ *)d;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00222}00222\ \ \ \ \ dv\ =\ (hila::scalar\_type<T>\ *)(\textcolor{keywordtype}{void}\ *)\&v;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00223}00223\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00224}00224\ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ N;\ ++i)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00225}00225\ \ \ \ \ \ \ \ \ atomic\_Add(dp\ +\ i,\ dv[i]);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00226}00226\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00227}00227\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00228}00228\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00230}00230\ \_\_device\_\_\ \textcolor{keyword}{inline}\ \textcolor{keywordtype}{double}\ atomicMultiply(\textcolor{keywordtype}{double}\ *dp,\ \textcolor{keywordtype}{double}\ v)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00231}00231\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00232}00232\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ *dp\_ull\ =\ (\textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ *)dp;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00233}00233\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ old\ =\ *dp\_ull;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00234}00234\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{long}\ \textcolor{keywordtype}{int}\ av;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00235}00235\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00236}00236\ \ \ \ \ \textcolor{keywordflow}{do}\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00237}00237\ \ \ \ \ \ \ \ \ av\ =\ old;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00238}00238\ \ \ \ \ \ \ \ \ old\ =\ atomicCAS(dp\_ull,\ av,\ \_\_double\_as\_longlong(v\ *\ \_\_longlong\_as\_double(av)));}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00239}00239\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00240}00240\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Note:\ uses\ integer\ comparison\ to\ avoid\ hang\ in\ case\ of\ NaN\ (since\ NaN\ !=\ NaN)}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00241}00241\ \ \ \ \ \}\ \textcolor{keywordflow}{while}\ (av\ !=\ old);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00242}00242\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00243}00243\ \ \ \ \ \textcolor{keywordflow}{return}\ \_\_longlong\_as\_double(old);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00244}00244\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00245}00245\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00247}00247\ \_\_device\_\_\ \textcolor{keyword}{inline}\ \textcolor{keywordtype}{float}\ atomicMultiply(\textcolor{keywordtype}{float}\ *dp,\ \textcolor{keywordtype}{float}\ v)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00248}00248\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{int}\ *dp\_ui\ =\ (\textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{int}\ *)dp;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00249}00249\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{int}\ old\ =\ *dp\_ui;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00250}00250\ \ \ \ \ \textcolor{keywordtype}{unsigned}\ \textcolor{keywordtype}{int}\ av;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00251}00251\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00252}00252\ \ \ \ \ \textcolor{keywordflow}{do}\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00253}00253\ \ \ \ \ \ \ \ \ av\ =\ old;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00254}00254\ \ \ \ \ \ \ \ \ old\ =\ atomicCAS(dp\_ui,\ av,\ \_\_float\_as\_int(v\ *\ \_\_int\_as\_float(av)));}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00255}00255\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00256}00256\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Note:\ uses\ integer\ comparison\ to\ avoid\ hang\ in\ case\ of\ NaN\ (since\ NaN\ !=\ NaN)}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00257}00257\ \ \ \ \ \}\ \textcolor{keywordflow}{while}\ (av\ !=\ old);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00258}00258\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00259}00259\ \ \ \ \ \textcolor{keywordflow}{return}\ \_\_int\_as\_float(old);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00260}00260\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00261}00261\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00263}00263\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00264}00264\ \textcolor{comment}{//\ NOTE:\ IF\ YOU\ DEFINE\ ALT\_VECTOR\_REDUCTION\ YOU\ NEED\ TO\ DEFINE\ THE\ SAME\ IN\ }}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00265}00265\ \textcolor{comment}{//\ codegen\_gpu.cpp!}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00266}00266\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00267}00267\ \textcolor{comment}{//\ \#define\ ALT\_VECTOR\_REDUCTION}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00268}00268\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00269}00269\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00270}00270\ \_\_global\_\_\ \textcolor{keywordtype}{void}\ sum\_blocked\_vectorreduction\_kernel(T\ *D,\ \textcolor{keyword}{const}\ \textcolor{keywordtype}{int}\ reduction\_size,}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00271}00271\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{const}\ \textcolor{keywordtype}{int}\ threads)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00272}00272\ \ \ \ \ \textcolor{keywordtype}{int}\ \textcolor{keywordtype}{id}\ =\ threadIdx.x\ +\ blockIdx.x\ *\ blockDim.x;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00273}00273\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00274}00274\ \ \ \ \ T\ sum;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00275}00275\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00276}00276\ \textcolor{preprocessor}{\#ifndef\ ALT\_VECTOR\_REDUCTION}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00277}00277\ \ \ \ \ \textcolor{keywordflow}{if}\ (\textcolor{keywordtype}{id}\ <\ reduction\_size)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00278}00278\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ id\ is\ now\ the\ reduction\ coordinate}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00279}00279\ \ \ \ \ \ \ \ \ sum\ =\ D[id];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00280}00280\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 1;\ i\ <\ threads;\ i++)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00281}00281\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ add\ everything\ to\ zero}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00282}00282\ \ \ \ \ \ \ \ \ \ \ \ \ sum\ +=\ D[\textcolor{keywordtype}{id}\ +\ i\ *\ reduction\_size];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00283}00283\ \ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00284}00284\ \ \ \ \ \ \ \ \ D[id]\ =\ sum;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00285}00285\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00286}00286\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00287}00287\ \textcolor{preprocessor}{\#else}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00288}00288\ \ \ \ \ \textcolor{keywordflow}{if}\ (\textcolor{keywordtype}{id}\ <\ reduction\_size)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00289}00289\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ id\ is\ now\ the\ reduction\ coordinate}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00290}00290\ \ \ \ \ \ \ \ \ sum\ =\ D[\textcolor{keywordtype}{id}\ *\ threads];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00291}00291\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 1;\ i\ <\ threads;\ i++)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00292}00292\ \ \ \ \ \ \ \ \ \ \ \ \ sum\ +=\ D[\textcolor{keywordtype}{id}\ *\ threads\ +\ i];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00293}00293\ \ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00294}00294\ \ \ \ \ \ \ \ \ D[\textcolor{keywordtype}{id}\ *\ threads]\ =\ sum;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00295}00295\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00296}00296\ \textcolor{preprocessor}{\#endif}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00297}00297\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00298}00298\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00299}00299\ \textcolor{preprocessor}{\#ifdef\ ALT\_VECTOR\_REDUCTION}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00300}00300\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00301}00301\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00302}00302\ \_\_global\_\_\ \textcolor{keywordtype}{void}\ sum\_blocked\_vectorreduction\_k2(T\ *D,\ \textcolor{keyword}{const}\ \textcolor{keywordtype}{int}\ reduction\_size,\ \textcolor{keyword}{const}\ \textcolor{keywordtype}{int}\ threads)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00303}00303\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00304}00304\ \ \ \ \ \textcolor{keywordtype}{int}\ \textcolor{keywordtype}{id}\ =\ threadIdx.x\ +\ blockIdx.x\ *\ blockDim.x;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00305}00305\ \ \ \ \ \textcolor{keywordflow}{if}\ (\textcolor{keywordtype}{id}\ <\ threads)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00306}00306\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (;\ \textcolor{keywordtype}{id}\ <\ reduction\_size;\ \textcolor{keywordtype}{id}\ +=\ threads)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00307}00307\ \ \ \ \ \ \ \ \ \ \ \ \ D[id]\ =\ D[\textcolor{keywordtype}{id}\ *\ threads];}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00308}00308\ \ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00309}00309\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00310}00310\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00311}00311\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00312}00312\ \textcolor{preprocessor}{\#endif}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00313}00313\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00314}00314\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00315}00315\ \textcolor{keywordtype}{void}\ sum\_blocked\_vectorreduction(T\ *data,\ \textcolor{keyword}{const}\ \textcolor{keywordtype}{int}\ reduction\_size,\ \textcolor{keyword}{const}\ \textcolor{keywordtype}{int}\ threads)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00316}00316\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00317}00317\ \ \ \ \ \textcolor{comment}{//\ straightforward\ implementation,\ use\ as\ many\ threads\ as\ elements\ in\ reduction\ vector}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00318}00318\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00319}00319\ \ \ \ \ \textcolor{keywordtype}{int}\ blocks\ =\ (reduction\_size\ +\ N\_threads\ -\/\ 1)\ /\ N\_threads;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00320}00320\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00321}00321\ \ \ \ \ sum\_blocked\_vectorreduction\_kernel<<<blocks,\ N\_threads>>>(data,\ reduction\_size,\ threads);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00322}00322\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00323}00323\ \textcolor{preprocessor}{\#ifdef\ ALT\_VECTOR\_REDUCTION}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00324}00324\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00325}00325\ \ \ \ \ sum\_blocked\_vectorreduction\_k2<<<1,\ N\_threads>>>(data,\ reduction\_size,\ threads);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00326}00326\ \textcolor{preprocessor}{\#endif}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00327}00327\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00328}00328\ \ \ \ \ check\_device\_error(\textcolor{stringliteral}{"{}sum\_blocked\_vectorreduction"{}});}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00329}00329\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00330}00330\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00331}00331\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00333}00333\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00334}00334\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00335}00335\ \_\_global\_\_\ \textcolor{keywordtype}{void}\ gpu\_set\_value\_kernel(T\ *vector,\ T\ value,\ \textcolor{keywordtype}{int}\ elems)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00336}00336\ \ \ \ \ \textcolor{keywordtype}{int}\ Index\ =\ threadIdx.x\ +\ blockIdx.x\ *\ blockDim.x;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00337}00337\ \ \ \ \ \textcolor{keywordflow}{if}\ (Index\ <\ elems)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00338}00338\ \ \ \ \ \ \ \ \ vector[Index]\ =\ value;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00339}00339\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00340}00340\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00341}00341\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00342}00342\ \textcolor{comment}{//\ template\ <typename\ T>}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00343}00343\ \textcolor{comment}{//\ \_\_global\_\_\ void\ gpu\_set\_zero\_kernel(T\ *vector,\ int\ elems)\ \{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00344}00344\ \textcolor{comment}{//\ \ \ \ \ unsigned\ Index\ =\ threadIdx.x\ +\ blockIdx.x\ *\ blockDim.x;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00345}00345\ \textcolor{comment}{//\ \ \ \ \ if\ (Index\ <\ elems)\ \{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00346}00346\ \textcolor{comment}{//\ \ \ \ \ \ \ \ \ vector[Index]\ =\ 0;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00347}00347\ \textcolor{comment}{//\ \ \ \ \ \}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00348}00348\ \textcolor{comment}{//\ \}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00349}00349\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00350}00350\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00352}00352\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00353}00353\ \textcolor{keyword}{inline}\ \textcolor{keywordtype}{void}\ gpu\_set\_zero(T\ *vec,\ \textcolor{keywordtype}{size\_t}\ N)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00354}00354\ \ \ \ \ gpuMemset(vec,\ 0,\ N\ *\ \textcolor{keyword}{sizeof}(T));}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00355}00355\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00356}00356\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00358}00358\ \textcolor{keyword}{template}\ <\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00359}00359\ \textcolor{keywordtype}{void}\ gpu\_set\_value(T\ *vec,\ \textcolor{keyword}{const}\ T\ \&val,\ \textcolor{keywordtype}{size\_t}\ N)\ \{}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00360}00360\ \ \ \ \ \textcolor{keywordtype}{int}\ blocks\ =\ N\ /\ N\_threads\ +\ 1;}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00361}00361\ \ \ \ \ gpu\_set\_value\_kernel<<<blocks,\ N\_threads>>>(vec,\ val,\ N);}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00362}00362\ \}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00363}00363\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00364}00364\ \textcolor{comment}{//\ template\ <typename\ T>}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00365}00365\ \textcolor{comment}{//\ void\ gpu\_set\_zero(T\ *vec,\ size\_t\ N)\ \{}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00366}00366\ \textcolor{comment}{//\ \ \ \ \ int\ blocks\ =\ N\ /\ N\_threads\ +\ 1;}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00367}00367\ \textcolor{comment}{//\ \ \ \ \ gpu\_set\_zero\_kernel<<<blocks,\ N\_threads>>>(vec,\ N);}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00368}00368\ \textcolor{comment}{//\ \}}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00369}00369\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00370}00370\ \textcolor{preprocessor}{\#endif\ }\textcolor{comment}{//\ !HILAPP}}
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00371}00371\ }
\DoxyCodeLine{\Hypertarget{gpu__templated__ops_8h_source_l00372}00372\ \textcolor{preprocessor}{\#endif}}

\end{DoxyCode}
