// Generated by Transformer at Thu Oct 10 18:46:05 2019

// start include "SUN.h"---------------------------------
#ifndef SUN_H_
#define SUN_H_

#include <iostream>
#include <string>
#include <math.h>

#define PI 3.14159265358979323846

#define NDIM 4
const int N=2;

// Include the lattice field definition
// start include "../plumbing/field.h"---------------------------------
// -*- mode: c++ -*-
#ifndef FIELD_H
#define FIELD_H
#include <iostream>
#include <string>
#include <math.h>
#include <type_traits>

// using namespace std;

// start include "../plumbing/globals.h"---------------------------------
#ifndef GLOBALS_H
#define GLOBALS_H

/// This file contains general global state of the simulation programs.
/// Probably this could be done much more elegantly

#include <iostream>
// start include "../plumbing/lattice.h"---------------------------------
#ifndef LATTICE_H
#define LATTICE_H

#include <iostream>
#include <fstream>
#include <array>
#include <vector>


// TODO: assertion moved somewhere where basic params
#undef NDEBUG
#include <assert.h>
// start include "../plumbing/defs.h"---------------------------------
#ifndef DEFS_H
#define DEFS_H

// Useful global definitions here -- this file should be included by (almost) all others

#include <array>
#include <vector>
#include <assert.h> 

#define EVENFIRST
#define layout_SOA

// TODO: default type real_t definition somewhere (makefile?)
using real_t = double;


#if defined(CUDA) || defined(openacc)
#include <cuda_runtime.h>
#define random() curand_uniform_double(&state);
#else
#define random() mersenne()
#endif


// move these somewhere - use consts?
// Have this defined in the program?
#ifndef NDIM
  #define NDIM 4
#endif

#ifndef USE_MPI
  #define VANILLA
#endif

// Direction and parity

#if   NDIM==4
enum direction :unsigned { XUP, YUP, ZUP, TUP, TDOWN, ZDOWN, YDOWN, XDOWN, NDIRS };
#elif NDIM==3
enum direction { XUP, YUP, ZUP, ZDOWN, YDOWN, XDOWN, NDIRS };
#elif NDIM==2
enum direction { XUP, YUP, YDOWN, XDOWN, NDIRS };
#elif NDIM==1
enum direction { XUP, XDOWN, NDIRS };
#endif

__device__ __host__ static inline direction opp_dir(const direction d) { return static_cast<direction>(NDIRS - 1 - static_cast<int>(d)); }
__device__ __host__ static inline direction opp_dir(const int d) { return static_cast<direction>(NDIRS - 1 - d); }

enum class parity : unsigned { none, even, odd, all, x };
// use here #define instead of const parity. Makes EVEN a protected symbol
const parity EVEN = parity::even;
const parity ODD  = parity::odd;
const parity ALL  = parity::all;
const parity X    = parity::x;

// turns EVEN <-> ODD, ALL remains.  X->none, none->none
__device__ __host__ static inline parity opp_parity(const parity p) {
  unsigned u = 0x3 & static_cast<unsigned>(p);
  return static_cast<parity>(0x3 & ((u<<1)|(u>>1)));
}

#define foralldir(d) for(int d=XUP; d<NDIM; d++)

__device__ __host__ static inline int is_up_dir(const int d) { return d<NDIM; }


// location type


using location = std::array<int,NDIM>;

__device__ __host__ inline location operator+(const location & a, const location & b) {
  location r;
  foralldir(d) r[d] = a[d] + b[d];
  return r;
}

__device__ __host__ inline location operator-(const location & a, const location & b) {
  location r;
  foralldir(d) r[d] = a[d] - b[d];
  return r;
}

__device__ __host__ inline location operator-(const location & a) {
  location r;
  foralldir(d) r[d] = -a[d];
  return r;
}

__device__ __host__ inline parity location_parity(const location & a) {
  int s = 0;
  foralldir(d) s += a[d];
  if (s % 2 == 0) return parity::even;
  else return parity::odd;
}


#ifndef USE_MPI
#define mynode() 0
#define numnodes() 1
__device__ __host__ inline void finishrun() {
  exit(1);
}
#endif

#define MAX_GATHERS 1000


__device__ __host__ inline void assert_even_odd_parity( parity p ) {
    assert(p == EVEN || p == ODD || p == ALL);
}

#endif
// end include "../plumbing/defs.h"---------------------------------

#include "../plumbing/memory.h"

struct node_info {
  location min,size;
  unsigned evensites, oddsites;
};
using location = std::array<int,NDIM>;


class lattice_struct {
private:
  // expose these directly, by far the simplest interface - who cares about c++ practices
  // use also ints instead of unsigned, just to avoid surprises in arithmetics
  // I shall assume here that int is 32 bits, and long long 64 bits.  I guess these are
  // pretty much standard for now
  // Alternative: int_32t and int_64t (or int_fast_32t  and int_fast_64t, even more generally) 
  int l_size[NDIM];
  long long l_volume;

  // Information about the node stored on this process
  struct node_struct {
    unsigned index;
    unsigned sites, evensites, oddsites;
    unsigned field_alloc_size;          // how many sites/node in allocations 
    location min, size;                 // node local coordinate ranges
    unsigned nn[NDIRS];                 // nn-node of node down/up to dirs
    bool first_site_even;               // is location min even or odd?
    std::vector<location> site_index_list;

    void setup(node_info & ni, lattice_struct & lattice);
  } this_node;

  // information about all nodes
  struct allnodes {
    unsigned number;
    unsigned ndir[NDIM];  // number of node divisions to dir
    // lattice division: div[d] will have num_dir[d]+1 elements, last size
    // TODO: is this needed at all?
    std::vector<unsigned> divisors[NDIM];
    std::vector<node_info> nodelist;

    unsigned * map_array;                  // mapping (optional)
    unsigned * map_inverse;                // inv of it
    
    void create_remap();                   // create remap_node
    unsigned remap(unsigned i);            // use remap
    unsigned inverse_remap(unsigned i);    // inverse remap
    
  } nodes;

  struct comm_struct {

  };
  
  std::vector<comm_struct> commlist;

  struct comm_node_struct {
    unsigned index;
    unsigned sites, evensites, oddsites;
    unsigned buffer;
    std::vector<unsigned>  sitelist;
  };

  struct comminfo_struct {
    int label;    
    unsigned * index;
    std::vector<comm_node_struct> from_node;
    std::vector<comm_node_struct> to_node;
  };
  
  std::vector<comminfo_struct> comminfo;

public:

  unsigned * neighb[NDIRS];
  
  void setup(int siz[NDIM]);
  void setup_layout();
  void setup_nodes();
  
  #if NDIM == 4
  void setup(int nx, int ny, int nz, int nt);
  #elif NDIM == 3  
  void setup(int nx, int ny, int nz);
  #elif NDIM == 2
  void setup(int nx, int ny);
  #elif NDIM == 1
  void setup(int nx); 
  #endif
  
  __device__ __host__ int size(direction d) { return l_size[d]; }
  __device__ __host__ int size(int d) { return l_size[d]; }
  __device__ __host__ long long volume() { return l_volume; }
  __device__ __host__ int node_number() { return this_node.index; }
  
  bool is_on_node(const location & c);
  unsigned node_number(const location & c);
  unsigned site_index(const location & c);
  unsigned site_index(const location & c, const unsigned node);
  location site_location(unsigned index);
  __device__ __host__ unsigned field_alloc_size() {return this_node.field_alloc_size; }
  void create_std_gathers();

  unsigned remap_node(const unsigned i);
  
  #ifdef EVENFIRST
  __device__ __host__ const int loop_begin( parity P){
    if(P==ODD){
      return this_node.evensites;
    } else {
      return 0;
    }
  }
  __device__ __host__ const int loop_end( parity P){
    if(P==EVEN){
      return this_node.evensites;
    } else {
      return this_node.sites;
    }
  }
  #else
  const int loop_begin( parity P){
    if(P==EVEN){
      return this_node.evensites;
    } else {
      return 0;
    }
  }
  const int loop_end( parity P){
    if(P==ODD){
      return this_node.evensites;
    } else {
      return this_node.sites;
    }
  }
  #endif


  /* Communication routines. Define here in lattice? */
  template <typename T>
  void reduce_node_sum(T value, bool distribute);

  template <typename T>
  void reduce_node_product(T value, bool distribute);
  
};

/// global handle to lattice
extern lattice_struct * lattice;

#ifdef USE_MPI
#include "comm_mpi.h"
#else
// start include "comm_vanilla.h"---------------------------------
#include "globals.h"
#include "field.h"

///***********************************************************
/// Vanilla (non-mpi) implementations of communication routines.
///

template <typename T>
__device__ __host__ void lattice_struct::reduce_node_sum(T value, bool distribute){}

template <typename T>
__device__ __host__ void lattice_struct::reduce_node_product(T value, bool distribute){}

//template<typename T>
//void field<T>::start_move(direction d, parity p){}

// end include "comm_vanilla.h"---------------------------------

#endif


#endif
// end include "../plumbing/lattice.h"---------------------------------


// text output section -- defines also output0, which writes from node 0 only

namespace hila {
  // this is our default output file stream
  extern std::ostream &output;
  // this is just a hook to store output file, if it is in use
  extern std::ofstream output_file;
};

// this is pretty hacky but easy.  Probably could do without #define too
// do this through else-branch in order to avoid if-statement problems
#define output0 if (mynode() != 0) {} else hila::output

#endif
// end include "../plumbing/globals.h"---------------------------------



// HACK  -- this is needed for pragma handlin, do not change!
// #pragma transformer _transformer_cmd_dump_ast_

// HACK
#define transformer_ctl(a) extern int _transformer_ctl_##a
//void transformer_control(const char *);


struct parity_plus_direction {
  parity p;
  direction d;
};

const parity_plus_direction operator+(const parity par, const direction d);
const parity_plus_direction operator-(const parity par, const direction d);

// This is a marker for transformer -- for does not survive as it is
#define onsites(p) for(parity parity_type_var_(p);;)

//class parity {
//  parity() {}
//  parity( parity &par ) { p = par.p; }
//  parity( parity_enum pare ) { p = pare; }
//  
//  parity & operator=(const parity &par) { p = par.p; return *this; }
//  parity & operator=(const parity_enum par) { p = par; return *this; }
//  const parity_plus_direction operator+(const direction &d) { parity_plus_direction pd; return(pd); }
//  const parity_plus_direction operator-(const direction &d) { parity_plus_direction pd; return(pd); }
//};

// #define onallsites(i) for (int i=0; i<N; i++) 


// fwd definition
// template <typename T> class field;

#if 0
// field_element class: virtual class, no storage allocated,
// wiped out by the transformer
template <typename T>
class field_element  {
 private:
  T v;   // TODO: this must be set appropriately?
  
 public:
  // the following are just placemarkers, and are substituted by the transformer

  // implicit conversion to type T: this works for storage types where field is an std
  // array of type T - this is again for type propagation
  // TODO: IS THIS NEEDED?  WE WANT TO AVOID CONVERSIONS FROM field<T> v -> T
  // operator T() { return v; }
      
  // The type is important for ensuring correctness
  // Possibility: write these so that they work without the transformer
  template <typename A,
            std::enable_if_t<std::is_assignable<T&,A>::value, int> = 0 >
  field_element<T>& operator= (const A &d) {
    v = d; 
    return *this;
  }
  
  // field_element = field_element
  field_element<T>& operator=  (const field_element<T>& rhs) {
    v  = rhs.v; return *this;}
  field_element<T>& operator+= (const field_element<T>& rhs) {
    v += rhs.v; return *this;}
  field_element<T>& operator-= (const field_element<T>& rhs) {
    v -= rhs.v; return *this;}
  field_element<T>& operator*= (const field_element<T>& rhs) {
    v *= rhs.v; return *this;}
  field_element<T>& operator/= (const field_element<T>& rhs) {
    v /= rhs.v; return *this;}

  //field_element<T>& operator+= (const T rhs) {
  //  v += rhs; return *this;}
  //field_element<T>& operator-= (const T rhs) {
  //  v -= rhs; return *this;}
  //field_element<T>& operator*= (const T rhs) {
  //  v *= rhs; return *this;}
  //field_element<T>& operator/= (const T rhs) {
  //  v /= rhs; return *this;}

  field_element<T>& operator+= (const double rhs) {
    v += rhs; return *this;}
  field_element<T>& operator-= (const double rhs) {
    v -= rhs; return *this;}
  field_element<T>& operator*= (const double rhs) {
    v *= rhs; return *this;}
  field_element<T>& operator/= (const double rhs) {
    v /= rhs; return *this;}


  // access the raw value - TODO:short vectors 
  T get_value() { return v; }

  T reduce_plus() {
    return v;   // TODO: short vector!
  }

  T reduce_mult() {
    return v;   // TODO: short vector!
  }
  
};

// declarations, implemented by transformer -- not necessarily defined anywhere
// +
template <typename T>
field_element<T> operator+( const field_element<T> &lhs, const field_element<T> &rhs);

//template <typename T>
//field_element<T> operator+( const T &lhs, const field_element<T> &rhs);

//template <typename T>
//field_element<T> operator+( const field_element<T> &lhs,  const T &rhs);

template <typename T,typename L>
field_element<T> operator+( const L &lhs, const field_element<T> &rhs);

template <typename T,typename R>
field_element<T> operator+( const field_element<T> &lhs, const R &rhs);

// -
template <typename T>
field_element<T> operator-( const field_element<T> &lhs, const field_element<T> &rhs);

template <typename T,typename L>
field_element<T> operator-( const L &lhs, const field_element<T> &rhs);

template <typename T,typename R>
field_element<T> operator-( const field_element<T> &lhs,  const R &rhs);

// template <typename T>
// field_element<T> operator-( double lhs, const field_element<T> &rhs);

// template <typename T>
// field_element<T> operator-( const field_element<T> &lhs, double rhs);

// *
template <typename T>
field_element<T> operator*( const field_element<T> &lhs, const field_element<T> &rhs);

template <typename T,typename L>
field_element<T> operator*( const L &lhs, const field_element<T> &rhs);

template <typename T,typename R>
field_element<T> operator*( const field_element<T> &lhs,  const R &rhs);

// template <typename T>
// field_element<T> operator*( double lhs, const field_element<T> &rhs);

// template <typename T>
// field_element<T> operator*( const field_element<T> &lhs, double rhs);


// /    Division is not implemented for all types, but define generically here
template <typename T>
field_element<T> operator/( const field_element<T> &lhs, const field_element<T> &rhs);

template <typename T,typename L>
field_element<T> operator/( const L &lhs, const field_element<T> &rhs);

template <typename T,typename R>
field_element<T> operator/( const field_element<T> &lhs,  const R &rhs);

// template <typename T>
// field_element<T> operator/( const field_element<T> &lhs, double rhs);



// a function
template <typename T>
field_element<T> exp( field_element<T> &arg) {
  field_element<T> res;
  res = exp(arg.get_value());
  return res;
}


// TRY NOW AUTOMATIC REDUCTION IDENTIFICATION
// Overload operator  res += expr, where
// res is type T and expr is field_element<T>
// Make these void, because these cannot be assigned from
// These will be modified by transformer

template <typename T>
void operator += (T& lhs, field_element<T>& rhs) {
  lhs += rhs.reduce_plus();
}

template <typename T>
void operator *= (T& lhs, field_element<T>& rhs) {
  lhs *= rhs.reduce_mult();
}

#endif

template <typename T>
using field_element = T;


// Type alias would be nice, but one cannot specialize those!
// Placemarker, will be specialized by transformer
template <typename T>
struct field_storage_type {
  T c;
};template<>
struct field_storage_type<class matrix<2, 2, struct cmplx<double> >> {
  class matrix<2, 2, struct cmplx<double> > c[10];
};


/// The following struct holds the data + information about the field
/// TODO: field-specific boundary conditions?
template <typename T>
class field_struct {
  private:
    constexpr static int t_elements = sizeof(T) / sizeof(real_t);
    field_storage_type<T> * payload; // TODO: must be maximally aligned, modifiers - never null
  public:
    real_t * fieldbuf;
    lattice_struct * lattice;
    unsigned is_fetched[NDIRS];

    #ifdef layout_SOA

    union T_access {
      T tu;
      real_t  tarr[t_elements];
    };

    #ifndef CUDA
    void allocate_payload(){
      fieldbuf = (real_t *) allocate_field_mem( t_elements*sizeof(real_t) * lattice->field_alloc_size() );
      #pragma acc enter data create(fieldbuf)
      if (fieldbuf == nullptr) {
        std::cout << "Failure in field memory allocation\n";
        exit(1);
      }
    }

    void free_payload() {
      #pragma acc exit data delete(fieldbuf)
      free_field_mem((void *)fieldbuf);
      fieldbuf = nullptr;
    }
    #else
    void allocate_payload(){
      cudaMalloc(
        (void **)&fieldbuf,
        t_elements*sizeof(real_t) * lattice->field_alloc_size()
      );
      if (fieldbuf == nullptr) {
        std::cout << "Failure in field memory allocation\n";
        exit(1);
      }
    }

    void free_payload() {
      cudaFree(fieldbuf);
      fieldbuf = nullptr;
    }
    #endif

    //inline T get(int idx) {
    //  T_access ta;
    //  for (int i=0; i<t_elements; i++) {
    //    ta.tarr[i] = (real_t) fieldbuf[i][idx];
    //  }
    //  T value = ta.tu;
    //  return value;
    //}

    //void set(T value, int idx) {
    //  T_access ta;
    //  ta.tu = value; 
    //  for (int i=0; i<t_elements; i++) {
    //    fieldbuf[i][idx] = (real_t) ta.tarr[i];
    //  }
    //}

    inline T get(const int idx) const {
      T value;
      real_t *value_f = static_cast<real_t *>(static_cast<void *>(&value));
      for (int i=0; i<(sizeof(T)/sizeof(real_t)); i++) {
         value_f[i] = fieldbuf[i*lattice->field_alloc_size() + idx];
      }
      return value; 
    }
    
    inline void set(T value, const int idx) {
      real_t *value_f = static_cast<real_t *>(static_cast<void *>(&value));
      for (int i=0; i<(sizeof(T)/sizeof(real_t)); i++) {
        fieldbuf[i*lattice->field_alloc_size() + idx] = value_f[i];
      }
    }


    #else

    void allocate_payload() {
      payload = (field_storage_type<T> *) 
                   allocate_field_mem(sizeof(T) * lattice->field_alloc_size());
              
      if (payload == nullptr) {
        std::cout << "Failure in field memory allocation\n";
        exit(1);
      }
    }

    void free_payload() {
      free_field_mem((void *)payload);
      payload = nullptr;
    }

    T get(const int i) const
    {
      return (T) (payload[i].c);
    }

    void set(T value, const int i) 
    {
      payload[i].c = value;
    }
    #endif
    
  };






// These are helpers, to make generic templates
// e.g. t_plus(A,B) gives the type of the operator a + b, where a is of type A and b B.
#define t_plus(A,B)  decltype(std::declval<A>() + std::declval<B>())
#define t_minus(A,B) decltype(std::declval<A>() - std::declval<B>())
#define t_mul(A,B)   decltype(std::declval<A>() * std::declval<B>())
#define t_div(A,B)   decltype(std::declval<A>() / std::declval<B>())




// ** class field

template <typename T>
class field {
private:

  static_assert( std::is_trivial<T>::value, "Field expects only trivial elements");
  
public:

  field_struct<T> * fs;
  
  field<T>() {
    // std::cout << "In constructor 1\n";
    fs = nullptr;             // lazy allocation on 1st use
  }
  
  // TODO: for some reason this straightforward copy constructor seems to be necessary, the
  // one below it does not catch implicit copying.  Try to understand why
  field<T>(const field<T>& other) {
    fs = nullptr;  // this is probably unnecessary
    (*this)[ALL] = other[X];
  }
    
  // copy constructor - from fields which can be assigned
  template <typename A,
            std::enable_if_t<std::is_convertible<A,T>::value, int> = 0 >  
  field<T>(const field<A>& other) {
    fs = nullptr;  // this is probably unnecessary
    (*this)[ALL] = other[X];
  }


  template <typename A,
            std::enable_if_t<std::is_convertible<A,T>::value, int> = 0 >  
  field<T>(const A& val) {
    fs = nullptr;
    // static_assert(!std::is_same<A,int>::value, "in int constructor");
    (*this)[ALL] = val;
  }
  
  // move constructor - steal the content
  field<T>(field<T>&& rhs) {
    // std::cout << "in move constructor\n";
    fs = rhs.fs;
    rhs.fs = nullptr;
  }

  ~field<T>() {
    free();
  }
    
  void allocate() {
    assert(fs == nullptr);
    if (lattice == nullptr) {
      // TODO: write to some named stream
      std::cout << "Can not allocate field variables before lattice.setup()\n";
      exit(1);  // TODO - more ordered exit?
    }
    fs = new field_struct<T>;
    fs->lattice = lattice;
    fs->allocate_payload();

    mark_changed(ALL);
  }

  void free() {
    if (fs != nullptr) {
      fs->free_payload();
      delete fs;
      fs = nullptr;
    }
  }

  bool is_allocated() { return (fs != nullptr); }
  
  // call this BEFORE the var is written to
  void mark_changed(const parity p) {
    if (fs == nullptr) allocate();
    else {
      char pc = static_cast<char>(p);
      assert(p == EVEN || p == ODD || p == ALL);
      unsigned up = 0x3 & (!(static_cast<unsigned>(opp_parity(p))));
      for (int i=0; i<NDIRS; i++) fs->is_fetched[i] &= up;
    }
  }
  
  void assert_is_initialized() {
    if (fs == nullptr) {
      std::cout << "field variable used before it is assigned to\n";
      exit(1);
    }
  }
  
  // Overloading [] 
  // placemarker, should not be here
  // T& operator[] (const int i) { return data[i]; }

  // these give the field_element -- WILL BE modified by transformer
  field_element<T>& operator[] (const parity p) const;
  field_element<T>& operator[] (const parity_plus_direction p);
  //{ 
  //  return (field_element<T>) *this;
  //}

  T get_value_at(int i)
  {
    return this->fs->get(i);
  }

  void set_value_at(T value, int i)
  {
    this->fs->set(value, i);
  }

  // fetch the element at this loc
  // T get(int i) const;
  
  // Overloading = - possible only if T = A is OK
  template <typename A, 
            std::enable_if_t<std::is_assignable<T&,A>::value, int> = 0 >
  field<T>& operator= (const field<A>& rhs) {
    (*this)[ALL] = rhs[X];
    return *this;
  }

  // same but without the field
  template <typename A, 
            std::enable_if_t<std::is_assignable<T&,A>::value, int> = 0 >
  field<T>& operator= (const A& d) {
    (*this)[ALL] = d;
    return *this;
  }
  
  // Do also move assignment
  field<T>& operator= (field<T> && rhs) {
    if (this != &rhs) {
      free();
      fs = rhs.fs;
      rhs.fs = nullptr;
    }
    return *this;
  }
  
  // is OK if T+A can be converted to type T
  template <typename A,
            std::enable_if_t<std::is_convertible<t_plus(T,A),T>::value, int> = 0>
  field<T>& operator+= (const field<A>& rhs) { 
    (*this)[ALL] += rhs[X]; return *this;
  }
  
  template <typename A,
            std::enable_if_t<std::is_convertible<t_minus(T,A),T>::value, int> = 0>  
  field<T>& operator-= (const field<A>& rhs) { 
    (*this)[ALL] -= rhs[X];
    return *this;
  }
  
  template <typename A,
            std::enable_if_t<std::is_convertible<t_mul(T,A),T>::value, int> = 0>
  field<T>& operator*= (const field<A>& rhs) {
    (*this)[ALL] *= rhs[X]; 
    return *this;
  }

  template <typename A,
            std::enable_if_t<std::is_convertible<t_div(T,A),T>::value, int> = 0>
  field<T>& operator/= (const field<A>& rhs) {
    (*this)[ALL] /= rhs[X];
    return *this;
  }

  template <typename A,
            std::enable_if_t<std::is_convertible<t_plus(T,A),T>::value, int> = 0>
  field<T>& operator+= (const A & rhs) { (*this)[ALL] += rhs; return *this;}

  template <typename A,
            std::enable_if_t<std::is_convertible<t_minus(T,A),T>::value, int> = 0>  
  field<T>& operator-= (const A & rhs) { (*this)[ALL] -= rhs; return *this;}

  template <typename A,
            std::enable_if_t<std::is_convertible<t_mul(T,A),T>::value, int> = 0>
  field<T>& operator*= (const A & rhs) { (*this)[ALL] *= rhs; return *this;}
  
  template <typename A,
            std::enable_if_t<std::is_convertible<t_div(T,A),T>::value, int> = 0>
  field<T>& operator/= (const A & rhs) { (*this)[ALL] /= rhs; return *this;}


  // Communication routines
  #ifndef USE_MPI
  void start_move(direction d, parity p){}
  #endif
};


// these operators rely on SFINAE, OK if field_t_plus(A,B) exists i.e. A+B is OK
/// operator +
template <typename A, typename B>
__device__ __host__ auto operator+( const field<A> &lhs, const field<B> &rhs) -> field<t_plus(A,B)>
{
  field <t_plus(A,B)> tmp;
  tmp[ALL] = lhs[X] + rhs[X];
  return tmp;
}

template <typename A, typename B>
__device__ __host__ auto operator+( const A &lhs, const field<B> &rhs) -> field<t_plus(A,B)>
{
  field<t_plus(A,B)> tmp;
  tmp[ALL] = lhs + rhs[X];
  return tmp;
}

template <typename A, typename B>
__device__ __host__ auto operator+( const field<A> &lhs, const B &rhs) -> field<t_plus(A,B)>
{
  field<t_plus(A,B)> tmp;
  tmp[ALL] = lhs[X] + rhs;
  return tmp;
}

/// operator -
template <typename A, typename B>
__device__ __host__ auto operator-( const field<A> &lhs, const field<B> &rhs) -> field<t_minus(A,B)>
{
  field <t_minus(A,B)> tmp;
  tmp[ALL] = lhs[X] - rhs[X];
  return tmp;
}

template <typename A, typename B>
__device__ __host__ auto operator-( const A &lhs, const field<B> &rhs) -> field<t_minus(A,B)>
{
  field<t_minus(A,B)> tmp;
  tmp[ALL] = lhs - rhs[X];
  return tmp;
}

template <typename A, typename B>
__device__ __host__ auto operator-( const field<A> &lhs, const B &rhs) -> field<t_minus(A,B)>
{
  field<t_minus(A,B)> tmp;
  tmp[ALL] = lhs[X] - rhs;
  return tmp;
}


/// operator *
template <typename A, typename B>
__device__ __host__ auto operator*( const field<A> &lhs, const field<B> &rhs) -> field<t_mul(A,B)>
{
  field <t_mul(A,B)> tmp;
  tmp[ALL] = lhs[X] * rhs[X];
  return tmp;
}

template <typename A, typename B>
__device__ __host__ auto operator*( const A &lhs, const field<B> &rhs) -> field<t_mul(A,B)>
{
  field<t_mul(A,B)> tmp;
  tmp[ALL] = lhs * rhs[X];
  return tmp;
}

template <typename A, typename B>
__device__ __host__ auto operator*( const field<A> &lhs, const B &rhs) -> field<t_mul(A,B)>
{
  field<t_mul(A,B)> tmp;
  tmp[ALL] = lhs[X] * rhs;
  return tmp;
}

/// operator /
template <typename A, typename B>
__device__ __host__ auto operator/( const field<A> &lhs, const field<B> &rhs) -> field<t_div(A,B)>
{
  field <t_div(A,B)> tmp;
  tmp[ALL] = lhs[X] / rhs[X];
  return tmp;
}

template <typename A, typename B>
__device__ __host__ auto operator/( const A &lhs, const field<B> &rhs) -> field<t_div(A,B)>
{
  field<t_div(A,B)> tmp;
  tmp[ALL] = lhs / rhs[X];
  return tmp;
}

template <typename A, typename B>
__device__ __host__ auto operator/( const field<A> &lhs, const B &rhs) -> field<t_div(A,B)>
{
  field<t_div(A,B)> tmp;
  tmp[ALL] = lhs[X] / rhs;
  return tmp;
}

// template <typename T>
// field<T> operator+( const double lhs, const field<T> &rhs) {
//   field<T> tmp;
//   tmp[ALL] = lhs + rhs[X];
//   return tmp;
// }

// template <typename T>
// field<T> operator+( const field<T> &lhs, const double rhs) {
  
//   return lhs;
// }

//field<T> operator*( const field<T> &lhs, const field<T> &rhs) {
//  return lhs;
//}


// template <typename T>
// class reduction {
// private:
//   field_element<T> value;
  
// public:
//   void sum(const field_element<T> & rhs) { value += rhs; }
//   void operator+=(const field_element<T> & rhs) { value += rhs; }
//   void product(const field_element<T> & rhs) { value *= rhs; }
//   void operator*=(const field_element<T> & rhs) { value *= rhs; }

//   // TODO - short vectors!
//   void max(const field_element<T> & rhs) { if (rhs > value) value = rhs; }
//   void min(const field_element<T> & rhs) { if (rhs < value) value = rhs; }

//   // TODO - short vectors!
//   T get() { return value.get_value(); }

//   // implicit conversion
//   operator T() { return get(); }
  
// };


#endif

// end include "../plumbing/field.h"---------------------------------

#include "../plumbing/mersenne.h"
// start include "../datatypes/general_matrix.h"---------------------------------
#ifndef MATRIX_H
#define MATRIX_H
#include<type_traits>
// start include "cmplx.h"---------------------------------
#ifndef CMPLX_H
#define CMPLX_H

// let's not include the std::complex
//#include <complex>
//#include <cmath>


template <typename T = double>
struct cmplx {
  T re,im;
  
  // assignment is automatically OK, by c-standard
  //   cmplx operator=(cmplx rhs) { 
  //     re = rhs.re; im = rhs.im; 
  //     return *this; 
  //   }
  cmplx<T>() = default;
  
  cmplx<T>(const cmplx<T> & a) =default;

  // constructor from single scalar value 
  template <typename scalar_t,
            std::enable_if_t<std::is_arithmetic<scalar_t>::value, int> = 0 >
  constexpr cmplx<T>(const scalar_t val): re(static_cast<T>(val)), im(static_cast<T>(0)) {}
  
  // constructor c(a,b)
//   template <typename A, typename B,
//             std::enable_if_t<std::is_arithmetic<A>::value, int> = 0,
//             std::enable_if_t<std::is_arithmetic<B>::value, int> = 0 >
//   constexpr cmplx<T>(const A & a, const B & b) {
//     re = static_cast<T>(a);
//     im = static_cast<T>(b);
//   }

  // constructor c(a,b)
  template <typename A, typename B,
            std::enable_if_t<std::is_arithmetic<A>::value, int> = 0,
            std::enable_if_t<std::is_arithmetic<B>::value, int> = 0 >
  constexpr cmplx<T>(const A & a, const B & b): re(static_cast<T>(a)), im(static_cast<T>(b)) {}

  
  ~cmplx<T>() =default;
  
  // automatic casting from cmplx<T> -> cmplx<A>
  // TODO: ensure this works if A is vector type!
  template <typename A>
  operator cmplx<A>() const { 
    return cmplx<A>( { static_cast<A>(re), static_cast<A>(im) });
  }

//   // assignment from std::complex<A>  TODO: perhaps remove?
//   template <typename A>
//   cmplx<T> & operator=(const std::complex<A> & c) {
//     re = static_cast<T>(c.real()); im = static_cast<T>(c.imag());
//     return *this;
//   }
  
  template <typename scalar_t,
            std::enable_if_t<std::is_arithmetic<scalar_t>::value, int> = 0 >
  cmplx<T> & operator=(scalar_t s) {
    re = static_cast<T>(s);
    im = 0.0;
    return *this;
  }
  
  T real() const { return re; }
  T imag() const { return im; }

  T norm() const { return re*re + im*im; }
  // TODO: make this work for vector type!  Not double
  
  //currently this gives a compilation error
  double abs() const { return sqrt(static_cast<double>(norm()) ); }
  double arg() const { return atan2(static_cast<double>(im),static_cast<double>(re)); }


  cmplx<T> conj() const { return cmplx<T>( { re, -im } ); }

  cmplx<T> polar(const T r, const T theta) { 
    return cmplx<T>( { r*cos(theta), r*sin(theta) } );
  }

  // unary + and -
  cmplx<T> operator+() const {return *this;}
  cmplx<T> operator-() const {return cmplx<T>(-re, -im); }

  
  cmplx<T> & operator+= (const cmplx<T> & lhs) {
    re += lhs.re;
    im += lhs.im;
    return *this;
  }

  // TODO: for avx vector too -- #define new template macro
  template <typename A,
            std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
  cmplx<T> & operator+= (const A & a) {
    re += static_cast<T>(a);
    return *this;
  }
  
  cmplx<T> & operator-= (const cmplx<T> & lhs) {
    re -= lhs.re;
    im -= lhs.im;
    return *this;
  }
  
  // TODO: for vector too
  template <typename A,
            std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
  cmplx<T> & operator-= (const A & a) {
    re -= static_cast<T>(a);
    return *this;
  }
  
  cmplx<T> & operator*= (const cmplx<T> & lhs) {
    re = re * lhs.re - im * lhs.im;
    im = im * lhs.re + re * lhs.im;
    return *this;
  }
  
  // TODO: for vector too
  template <typename A,
            std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
  cmplx<T> & operator*= (const A & a) {
    re *= static_cast<T>(a);
    im *= static_cast<T>(a);
    return *this;
  }

  // a/b = a b*/|b|^2 = (a.re*b.re + a.im*b.im + i(a.im*b.re - a.re*b.im))/|b|^2
  cmplx<T> & operator/= (const cmplx<T> & lhs) {
    T n = lhs.norm();
    re = (re * lhs.re + im * lhs.im)/n;
    im = (im * lhs.re - re * lhs.im)/n;
    return *this;
  }
  
  // TODO: for vector too
  template <typename A,
            std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
  cmplx<T> & operator/= (const A & a) {
    re /= static_cast<T>(a);
    im /= static_cast<T>(a);
    return *this;
  }
};

template <typename T>
__device__ __host__ __device__ __host__ cmplx<T> operator+(const cmplx<T> & a, const cmplx<T> & b) {
  return cmplx<T>(a.re + b.re, a.im + b.im);
}

  // TODO: for avx vector too -- #define new template macro
template <typename T, typename A,
          std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
__device__ __host__ cmplx<T> operator+(const cmplx<T> & c, const A & a) {
  return cmplx<T>(c.re + a, c.im);
}

template <typename T, typename A,
          std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
__device__ __host__ cmplx<T> operator+(const A &a, const cmplx<T> & c) {
  return cmplx<T>(c.re + a, c.im);
}

// -
template <typename T>
__device__ __host__ cmplx<T> operator-(const cmplx<T> & a, const cmplx<T> & b) {
  return cmplx<T>(a.re - b.re, a.im - b.im);
}

// TODO: for avx vector too -- #define new template macro
template <typename T, typename A,
          std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
__device__ __host__ cmplx<T> operator-(const cmplx<T> & c, const A & a) {
  return cmplx<T>(c.re - a, c.im);
}

template <typename T, typename A,
          std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
__device__ __host__ cmplx<T> operator-(const A &a, const cmplx<T> & c) {
  return cmplx<T>(a - c.re, -c.im);
}


// *
template <typename T>
__device__ __host__ __device__ __host__ cmplx<T> operator*(const cmplx<T> & a, const cmplx<T> & b) {
  return cmplx<T>(a.re*b.re - a.im*b.im, a.im*b.re + a.re*b.im);
}

// TODO: for avx vector too -- #define new template macro
template <typename T, typename A,
          std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
__device__ __host__ cmplx<T> operator*(const cmplx<T> & c, const A & a) {
  return cmplx<T>(c.re * a, c.im * a);
}

template <typename T, typename A,
          std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
__device__ __host__ __device__ __host__ cmplx<T> operator*(const A &a, const cmplx<T> & c) {
  return cmplx<T>(a * c.re, a * c.im);
}


// /   a/b = ab*/|b|^2
template <typename T>
__device__ __host__ cmplx<T> operator/(const cmplx<T> & a, const cmplx<T> & b) {
  T n = b.norm();
  return cmplx<T>( (a.re*b.re + a.im*b.im)/n, (a.im*b.re - a.re*b.im)/n );
}

// TODO: for avx vector too -- #define new template macro
template <typename T, typename A,
          std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
__device__ __host__ cmplx<T> operator/(const cmplx<T> & c, const A & a) {
  return cmplx<T>(c.re / a, c.im / a);
}

// a/c = ac*/|c|^2 
template <typename T, typename A,
          std::enable_if_t<std::is_arithmetic<A>::value, int> = 0 >
__device__ __host__ cmplx<T> operator/(const A &a, const cmplx<T> & c) {
  T n = c.norm();
  return cmplx<T>((a * c.re)/n, -(a * c.im)/n);
}


// Operators to implement imaginary unit 1_i, enablig expressions  3 + 2_i  etc.
// Underscore seems to be required here

__device__ __host__ constexpr cmplx<double> operator""_i(long double a) {
  return cmplx<double>{0.0,a};
}

__device__ __host__ constexpr cmplx<double> operator""_i(unsigned long long a) {
  return cmplx<double>(0.0,static_cast<double>(a));
}

template <typename T>
__device__ __host__ std::ostream& operator<<(std::ostream &strm, const cmplx<T> A) {
  return strm << "(" << A.re << ", " << A.im << ")";
}

#endif

// end include "cmplx.h"---------------------------------


//--- conjugation for general type T -> template specialized for complex types

template<typename T>
__device__ __host__ inline T typeConj(T val){
  return val; 
}

template<typename Accuracy>
__device__ __host__ __device__ __host__ inline cmplx<Accuracy> typeConj(cmplx<Accuracy> val){
  return val.conj();
}

//---

template <const int n, const int m, typename T>
class matrix {
  public:
  T c[n][m];

  matrix() = default;

  template <typename scalart, std::enable_if_t<std::is_arithmetic<scalart>::value, int> = 0 >  
  matrix<n,m,T> & operator= (const scalart rhs) {
    static_assert(n==m, "rowdim != coldim : cannot assign diagonal from scalar!");
    for (int i=0; i<n; i++) for (int j=0; j<n; j++) {
      if (i == j) c[i][j] = (rhs);
      else c[i][j] = (0);
    }
    return *this;
  }


  //copy constructor from scalar  
  template <typename scalart, std::enable_if_t<std::is_arithmetic<scalart>::value, int> = 0 >  
  matrix(const scalart rhs) {
    static_assert(n==m, "rowdim != coldim : cannot assign diagonal from scalar!");
    for (int i=0; i<n; i++) for (int j=0; j<n; j++) {
      if (i == j) c[i][j] = (rhs);
      else c[i][j] = (0);
    }
  }

  //*=, +=, -= operators
  matrix<n,m,T> & operator+=(const matrix<n,m,T> & rhs){
    for (int i = 0; i < n; i++) for (int j = 0; j < m; j++){
      c[i][j] += rhs.c[i][j]; 
    }
    return *this;
  }

  matrix<n,m,T> & operator-=(const matrix<n,m,T> & rhs){
    for (int i = 0; i < n; i++) for (int j = 0; j < m; j++){
      c[i][j] -= rhs.c[i][j]; 
    }
    return *this;
  }

  template <typename scalart, std::enable_if_t<std::is_arithmetic<scalart>::value, int> = 0 >
  matrix<n,m,T> & operator*=(const scalart rhs){
    T val;
    val=rhs;
    for (int i = 0; i < n; i++) for (int j = 0; j < m; j++){
      c[i][j]*=val;
    }
    return *this;
  }

  template<int p>
  matrix<n,m,T> & operator*=(const matrix<m,p,T> & rhs){
    static_assert(m==p, "can't assign result of *= to matrix A, because doing so would change it's dimensions");
    matrix<m,m,T> rhsTrans = rhs.transpose();
    matrix<n,m,T> res;
    for (int i = 0; i < n; i++) for (int j = 0; j < m; j++){
      res.c[i][j] = (0);
      for (int k = 0; k < m; k++){
        res.c[i][j] += (c[i][k] * rhsTrans.c[j][k]);
      }
    }
    for (int i = 0; i < n; i++) for (int j = 0; j < m; j++){
      c[i][j] = res.c[i][j];
    }
    return *this;
  }

  //numpy style matrix fill 
  template <typename scalart, std::enable_if_t<std::is_arithmetic<scalart>::value, int> = 0 > 
  matrix<n,m,T> & fill(const scalart rhs) {
    for (int i = 0; i < n; i++) for (int j = 0; j < n; j++){
      c[i][j] = (rhs);
    }
    return *this;
  }
  
  //return copy of transpose of this matrix
  matrix<m,n,T> transpose() const {
    matrix<m,n,T> res;
    for (int i=0; i<m; i++) for (int j=0; j<n; j++) {
      res.c[i][j] =  c[j][i];
    }
    return res;
  }

  //return copy of complex conjugate of this matrix
  matrix<m,n,T> conjugate() const {
    matrix<m,n,T> res;
    for (int i=0; i<m; i++) for (int j=0; j<n; j++) {
      res.c[i][j] =  typeConj(c[j][i]);
    }
    return res;
  }

  T trace() const {
    static_assert(n==m, "trace not defined for non square matrices!");
    T result = static_cast<T>(0);
    for (int i = 0; i < n; i++){
      result += c[i][i];
    }
    return result;
  }

  std::string str() const {
    std::string text = "";
    for (int i=0; i<n; i++){
      for (int j=0; j<n; j++) {
        text + c[i][j].str() + " "; 
      }
      text + "\n"; 
    }
    return text;
  }
};

//templates needed for naive calculation of determinants

template<int n, int m, typename T>
__device__ __host__ matrix<n - 1, m - 1, T> Minor(const matrix<n, m, T> & bigger, int i, int j){
  matrix<n - 1, m - 1, T> result;
  int index = 0;
  for (int p = 0; p < n; p++) for (int l = 0; l < m; l++){
    if (p==i || l==j) continue;
    *(*(result.c) + index) = bigger.c[p][l];
    index++;
  }
  return result;
}

template<int n, int m, typename T>
__device__ __host__ T det(const matrix<n, m, T> & mat){
  static_assert(n==m, "determinants defined only for square matrices");
  T result = 1.0;
  T parity = 1.0; //assumes that copy constructor from scalar has been defined for T 
  T opposite = -1.0; 
  for (int i = 0; i < n; i++){
    matrix<n - 1, m - 1, T> minor = Minor(mat, 0, i);
    result += parity*det(minor)*mat.c[0][i];
    parity*=opposite;
  }
  return result;
}

template<typename T>
__device__ __host__ T det(const matrix<2,2,T> & mat){
  return mat.c[0][0]*mat.c[1][1] - mat.c[1][0]*mat.c[0][1];
}

//matrix multiplication for 2 by 2 matrices ; 
template<typename T>
__device__ __host__ __device__ __host__ matrix<2,2,T> operator* (const matrix<2,2,T> &A, const matrix<2,2,T> &B) {
  matrix<2,2,T> res = 1;
  res.c[0][0] = A.c[0][0]*B.c[0][0] + A.c[0][1]*B.c[1][0];
  res.c[0][1] = A.c[0][0]*B.c[0][1] + A.c[0][1]*B.c[1][1];
  res.c[1][1] = A.c[1][0]*B.c[0][1] + A.c[1][1]*B.c[1][1];
  res.c[1][0] = A.c[1][0]*B.c[0][0] + A.c[1][1]*B.c[1][0];
  return res;
}

//general naive matrix multiplication 
template <int n, int m, int p, typename T>
__device__ __host__ matrix<n,p,T> operator* (const matrix<n,m,T> &A, const matrix<m,p,T> &B) {
  matrix<p,m,T> Btrans = B.transpose(); //do matrix multiplication on rows of transpose matrix (should reduce cache misses)
  matrix<n,p,T> res;
  for (int i = 0; i < n; i++) for (int j = 0; j < p; j++){
    res.c[i][j] = (0);
    for (int k = 0; k < m; k++){
      res.c[i][j] += (A.c[i][k] * Btrans.c[j][k]);
    }
  }
  return res;
}

//dot product definitions for vectors
template<int n, typename T>
__device__ __host__ T operator* (const matrix<1, n, T> & vecA, const matrix<n, 1, T> & vecB) {
  T result = (0.0);
  for (int i = 0; i < n; i++){
    result += vecA.c[0][i]*(typeConj(vecB.c[i][0]));
  }
  return result;
}

template<int n, typename T>
__device__ __host__ T operator* (const matrix<1, n, T> & vecA, const matrix<1, n, T> & vecB) {
  T result = (0.0);
  for (int i = 0; i < n; i++){
    result += vecA.c[0][i]*(typeConj(vecB.c[0][i]));
  }
  return result;
}

template<int n, typename T>
__device__ __host__ T operator* (const matrix<n, 1, T> & vecA, const matrix<n, 1, T> & vecB) {
  T result; 
  result=0;
  for (int i = 0; i < n; i++){
    result += vecA.c[i][0]*(typeConj(vecB.c[i][0]));
  }
  return result;
}

//component wise addition
template <int n, int m, typename T>
__device__ __host__ matrix<n,m,T> operator+ (const matrix<n,m,T> &A, const matrix<n,m,T> &B) {
  matrix<n,m,T> res;
  for (int i=0; i<n; i++) for (int j=0; j<n; j++) {
    res.c[i][j] =  A.c[i][j] + B.c[i][j];
  }
  return res;
}

//component wise subtraction
template <int n, int m, typename T>
__device__ __host__ matrix<n,m,T> operator- (const matrix<n,m,T> &A, const matrix<n,m,T> &B) {
  matrix<n,m,T> res;
  for (int i=0; i<n; i++) for (int j=0; j<n; j++) {
    res.c[i][j] =  A.c[i][j] - B.c[i][j];
  }
  return res;
}

// multiplication by a scalar
template <int n, int m, typename T, typename scalart, std::enable_if_t<std::is_arithmetic<scalart>::value, int> = 0 >
__device__ __host__ __device__ __host__ matrix<n,m,T> operator* (const matrix<n,m,T> &A, const scalart s) {
  matrix<n,m,T> res;
  for (int i=0; i<n; i++) for (int j=0; j<n; j++) {
    res.c[i][j] = s * A.c[i][j];
  }
  return res;
}

template <int n, int m, typename T, typename scalart, std::enable_if_t<std::is_arithmetic<scalart>::value, int> = 0 >
__device__ __host__ matrix<n,m,T> operator/ (const matrix<n,m,T> &A, const scalart s) {
  matrix<n,m,T> res;
  for (int i=0; i<n; i++) for (int j=0; j<n; j++) {
    res.c[i][j] = s / A.c[i][j];
  }
  return res;
}

template <int n, int m, typename T, typename scalart, std::enable_if_t<std::is_arithmetic<scalart>::value, int> = 0 >
__device__ __host__ __device__ __host__ matrix<n,m,T> operator*(const scalart s, const matrix<n,m,T> &A) {
  return operator*(A,s);
}

template <int n, int m, typename T>
__device__ __host__ std::ostream& operator<<(std::ostream &strm, const matrix<n,m,T> &A) {
  for (int i=0; i<n; i++){
    strm << "\n"; 
    for (int j=0; j<n; j++) {
      strm << " " << A.c[i][j] << " "; 
    }
    strm << "\n"; 
  }
  strm << "\n"; 
  return strm;
}

#endif
// end include "../datatypes/general_matrix.h"---------------------------------



// Define some parameters for the simulation
extern double beta;
extern int n_measurements;
extern int n_updates_per_measurement;
extern long seed;
extern int NX, NY, NZ, NT;
extern int VOLUME;

#pragma acc routine seq
double monte(
  matrix<N,N,cmplx<double>> &U, 
  matrix<N,N,cmplx<double>> &staple,
  double beta);

#pragma acc routine seq
void KennedyPendleton(
  matrix<2,2,cmplx<double>> &U,
  matrix<2,2,cmplx<double>> &staple
);


#endif //SUN_H_// end include "SUN.h"---------------------------------


// Direct output to stdout
std::ostream &hila::output = std::cout;
std::ostream &output = std::cout;

// Define the lattice global variable
lattice_struct my_lattice;
lattice_struct * lattice = & my_lattice;


// Define some parameters for the simulation
double beta = 8;
int n_measurements=100;
int n_updates_per_measurement=10;
long seed = 123456;
int NX=8, NY=8, NZ=8, NT=8;
int VOLUME = NX*NY*NZ*NT;



//----------
__global__ void kernel_calc_staples_1214(field_struct<matrix<2, 2, cmplx<double> > > * Fstaple_sum, const field_struct<matrix<N, N, cmplx<double> > > * Fdown_staple, const int dir2)
{
  int Index = threadIdx.x + blockIdx.x * blockDim.x  + lattice->loop_begin(parity::all); 
  if(Index < lattice->loop_end(parity::all)) { 
    matrix<2, 2, cmplx<double> > Fstaple_sum_index = Fstaple_sum->get(Index);
    matrix<N, N, cmplx<double> > Fdown_staple_index_dir2 = Fdown_staple->get(lattice->neighb[dir2][Index]);
    matrix<N, N, cmplx<double> > Fdown_staple_index = Fdown_staple->get(Index);
    Fstaple_sum_index += Fdown_staple_index_dir2;
    Fstaple_sum->set(Fstaple_sum_index, Index);
  }
}
//----------
//----------
__global__ void kernel_calc_staples_1018(field_struct<matrix<2, 2, cmplx<double> > > * Fstaple_sum, const field_struct<matrix<N, N, cmplx<double> > > * FU_dir2_, const field_struct<matrix<N, N, cmplx<double> > > * FU_dir_, const int dir, const int dir2)
{
  int Index = threadIdx.x + blockIdx.x * blockDim.x  + lattice->loop_begin(parity::all); 
  if(Index < lattice->loop_end(parity::all)) { 
    matrix<2, 2, cmplx<double> > Fstaple_sum_index = Fstaple_sum->get(Index);
    matrix<N, N, cmplx<double> > FU_dir2__index_dir = FU_dir2_->get(lattice->neighb[dir][Index]);
    matrix<N, N, cmplx<double> > FU_dir2__index = FU_dir2_->get(Index);
    matrix<N, N, cmplx<double> > FU_dir__index_dir2 = FU_dir_->get(lattice->neighb[dir2][Index]);
    matrix<N, N, cmplx<double> > FU_dir__index = FU_dir_->get(Index);
    Fstaple_sum_index += FU_dir2__index_dir
    * FU_dir__index_dir2.conjugate()
    * FU_dir2__index.conjugate();
    Fstaple_sum->set(Fstaple_sum_index, Index);
  }
}
//----------
//----------
__global__ void kernel_calc_staples_853(field_struct<matrix<2, 2, cmplx<double> > > * Fdown_staple, const field_struct<matrix<2, 2, cmplx<double> > > * FU_dir2_, const field_struct<matrix<2, 2, cmplx<double> > > * FU_dir_, const int dir)
{
  int Index = threadIdx.x + blockIdx.x * blockDim.x  + lattice->loop_begin(parity::all); 
  if(Index < lattice->loop_end(parity::all)) { 
    matrix<2, 2, cmplx<double> > Fdown_staple_index;matrix<2, 2, cmplx<double> > FU_dir2__index_dir = FU_dir2_->get(lattice->neighb[dir][Index]);
    matrix<2, 2, cmplx<double> > FU_dir2__index = FU_dir2_->get(Index);
    matrix<2, 2, cmplx<double> > FU_dir__index = FU_dir_->get(Index);
    Fdown_staple_index = FU_dir2__index.conjugate()
    * FU_dir__index
    * FU_dir2__index_dir;
    Fdown_staple->set(Fdown_staple_index, Index);
  }
}
//----------
//----------
__global__ void kernel_calc_staples_676(field_struct<matrix<2, 2, cmplx<double> > > * Fstaple_sum)
{
  int Index = threadIdx.x + blockIdx.x * blockDim.x  + lattice->loop_begin(parity::all); 
  if(Index < lattice->loop_end(parity::all)) { 
    matrix<2, 2, cmplx<double> > Fstaple_sum_index;Fstaple_sum_index = 0;
    Fstaple_sum->set(Fstaple_sum_index, Index);
  }
}
//----------

field<matrix<N,N,cmplx<double>>>
calc_staples( field<matrix<N,N,cmplx<double>>> U[NDIM], direction dir)
{
  /* Calculate the sum of staples connected to links in direction
   * dir 
   */
  field<matrix<N,N,cmplx<double>>> down_staple, staple_sum;
  //--  staple_sum[ALL] = 0;
  {
    field<matrix<2, 2, cmplx<double> > > & Fstaple_sum = staple_sum;
    Fstaple_sum.mark_changed(parity::all);
    kernel_calc_staples_676<<< 128, 128 >>>(Fstaple_sum.fs);
  }
  //----------
  foralldir(d2){
    direction dir2 = (direction)d2;
    //Calculate the down side staple.
    //This will be communicated up.
    output0 << "Loop 1";
    //--  down_staple[ALL] = U[dir2][X].conjugate()
    //--                       * U[dir][X]
    //--                       * U[dir2][X+dir];
    {
      field<matrix<2, 2, cmplx<double> > > & Fdown_staple = down_staple;
      field<matrix<2, 2, cmplx<double> > > & FU_dir2_ = U[dir2];
      field<matrix<2, 2, cmplx<double> > > & FU_dir_ = U[dir];
      assert(FU_dir2_.is_allocated() && FU_dir_.is_allocated());
      FU_dir2_.start_move(dir, parity::all);
      Fdown_staple.mark_changed(parity::all);
      kernel_calc_staples_853<<< 128, 128 >>>(Fdown_staple.fs, FU_dir2_.fs, FU_dir_.fs, dir);
    }
    //----------
    // Forward staple
    output0 << "Loop 2";
    //--  staple_sum[ALL] += U[dir2][X+dir]
    //--                       * U[dir][X+dir2].conjugate()
    //--                       * U[dir2][X].conjugate();
    {
      field<matrix<2, 2, cmplx<double> > > & Fstaple_sum = staple_sum;
      field<matrix<N, N, cmplx<double> > > & FU_dir2_ = U[dir2];
      field<matrix<N, N, cmplx<double> > > & FU_dir_ = U[dir];
      assert(Fstaple_sum.is_allocated() && FU_dir2_.is_allocated() && FU_dir_.is_allocated());
      FU_dir2_.start_move(dir, parity::all);
      FU_dir_.start_move(dir2, parity::all);
      Fstaple_sum.mark_changed(parity::all);
      kernel_calc_staples_1018<<< 128, 128 >>>(Fstaple_sum.fs, FU_dir2_.fs, FU_dir_.fs, dir, dir2);
    }
    //----------
    // Add the two staples together
    output0 << "Loop 3";
    //--  staple_sum[ALL] += down_staple[X - dir2];
    {
      field<matrix<2, 2, cmplx<double> > > & Fstaple_sum = staple_sum;
      field<matrix<N, N, cmplx<double> > > & Fdown_staple = down_staple;
      assert(Fstaple_sum.is_allocated() && Fdown_staple.is_allocated());
      Fdown_staple.start_move(dir2, parity::all);
      Fstaple_sum.mark_changed(parity::all);
      kernel_calc_staples_1214<<< 128, 128 >>>(Fstaple_sum.fs, Fdown_staple.fs, dir2);
    }
    //----------
  }
  return staple_sum;
}

 
template<typename T>
__device__ __host__ void update(
  T &U, const T &staple,
  double beta
){
  monte( U, staple, beta );
}

__device__ __host__ void update(
  matrix<2,2,cmplx<double>> &U,
  const matrix<2,2,cmplx<double>> &staple,
  double beta
){
  matrix<2,2,cmplx<double>> temp = -beta*staple;
  KennedyPendleton( U, temp );
}

//----------
__global__ void kernel_main_2750(const field_struct<matrix<2, 2, cmplx<double> > > * FU_dir1_, const field_struct<matrix<N, N, cmplx<double> > > * FU_dir2_, double & sv__0_, const int dir2, const int dir1)
{
  int Index = threadIdx.x + blockIdx.x * blockDim.x  + lattice->loop_begin(parity::all); 
  if(Index < lattice->loop_end(parity::all)) { 
    matrix<2, 2, cmplx<double> > FU_dir1__index_dir2 = FU_dir1_->get(lattice->neighb[dir2][Index]);
    matrix<2, 2, cmplx<double> > FU_dir1__index = FU_dir1_->get(Index);
    matrix<N, N, cmplx<double> > FU_dir2__index_dir1 = FU_dir2_->get(lattice->neighb[dir1][Index]);
    matrix<N, N, cmplx<double> > FU_dir2__index = FU_dir2_->get(Index);
    {
      matrix<N,N,cmplx<double>> temp;
      temp =  FU_dir1__index * FU_dir2__index_dir1;
      temp *= FU_dir1__index_dir2.conjugate();
      temp *= FU_dir2__index.conjugate();
      sv__0_ += 1-temp.trace().re/N;
    }
  }
}
//----------
//----------
__global__ void kernel_main_2429(const parity Parity, field_struct<matrix<2, 2, cmplx<double> > > * FU_dir_, const field_struct<matrix<2, 2, cmplx<double> > > * Fstaple, const double sv__0_)
{
  int Index = threadIdx.x + blockIdx.x * blockDim.x  + lattice->loop_begin(Parity); 
  if(Index < lattice->loop_end(Parity)) { 
    matrix<2, 2, cmplx<double> > FU_dir__index = FU_dir_->get(Index);
    matrix<2, 2, cmplx<double> > Fstaple_index = Fstaple->get(Index);
    {
      update( FU_dir__index, Fstaple_index, sv__0_ );
    }
    FU_dir_->set(FU_dir__index, Index);
  }
}
//----------
//----------
__global__ void kernel_main_1885(field_struct<matrix<2, 2, cmplx<double> > > * FU_d_)
{
  int Index = threadIdx.x + blockIdx.x * blockDim.x  + lattice->loop_begin(parity::all); 
  if(Index < lattice->loop_end(parity::all)) { 
    matrix<2, 2, cmplx<double> > FU_d__index;FU_d__index = 1;
    FU_d_->set(FU_d__index, Index);
  }
}
//----------

int main()
{
  // Basic setup
  lattice->setup( NX, NY, NZ, NT );
  // Define a field
  field<matrix<N,N,cmplx<double>>> U[NDIM];

  seed_mersenne( seed );

  /* "Warm up" the rng generator */
  for( int i=0; i<543210; i++ ) mersenne();
  
  // Set to 1
  foralldir(d) {
    output0 << "Init loop";
    //--  U[d][ALL] = 1;
    {
      field<matrix<2, 2, cmplx<double> > > & FU_d_ = U[d];
      FU_d_.mark_changed(parity::all);
      kernel_main_1885<<< 128, 128 >>>(FU_d_.fs);
    }
    //----------
  }

  // Run update-measure loop
  for( int i=0; i<n_measurements; i++ ){

    // Run a number of updates
    for(int j=0; j<n_updates_per_measurement; j++){
      foralldir(d) {
        // update direction dir
        direction dir = (direction)d;
        // First we need the staple sum
        field<matrix<N,N,cmplx<double>>> staple = calc_staples(U, dir);

        // Now update, first even then odd
        parity p = EVEN;
        for( int par=0; par < 2; par++ ){
          output0 << "Update loop";
          //--  onsites(p) {
          //--              update( U[dir][X], staple[X], beta );
          //--            }
          {
            const parity Parity = (p);
            field<matrix<2, 2, cmplx<double> > > & FU_dir_ = U[dir];
            field<matrix<2, 2, cmplx<double> > > & Fstaple = staple;
            assert(FU_dir_.is_allocated() && Fstaple.is_allocated());
            FU_dir_.mark_changed(Parity);
            kernel_main_2429<<< 128, 128 >>>(Parity, FU_dir_.fs, Fstaple.fs, beta);
          }
          //----------
          p = opp_parity(p);
        }
      }
    }

    // Measure plaquette
    double Plaq=0;
    foralldir(d1) foralldir(d2) if(d1 != d2){
      direction dir1 = (direction)d1, dir2 = (direction)d2;
      output0 << "Plaquette loop";
      //--  onsites(ALL) {
      //--          matrix<N,N,cmplx<double>> temp;
      //--          temp =  U[dir1][X] * U[dir2][X+dir1];
      //--          temp *= U[dir1][X+dir2].conjugate();
      //--          temp *= U[dir2][X].conjugate();
      //--          Plaq += 1-temp.trace().re/N;
      //--        }
      {
        field<matrix<2, 2, cmplx<double> > > & FU_dir1_ = U[dir1];
        field<matrix<N, N, cmplx<double> > > & FU_dir2_ = U[dir2];
        assert(FU_dir1_.is_allocated() && FU_dir2_.is_allocated());
        FU_dir1_.start_move(dir2, parity::all);
        FU_dir2_.start_move(dir1, parity::all);
        kernel_main_2750<<< 128, 128 >>>(FU_dir1_.fs, FU_dir2_.fs, Plaq, dir2, dir1);
        lattice->reduce_node_sum(Plaq, true);
      }
      //----------
    }
    printf("Plaquette %f\n", Plaq/(VOLUME*NDIM*(NDIM-1)));
  }
  
  return 0;
}
